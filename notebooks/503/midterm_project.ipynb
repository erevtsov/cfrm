{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6a822f05-c026-4a44-aef1-bc8ef758878f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import cvxpy as cp\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6a5cf3-2e03-4138-9183-05087b15086c",
   "metadata": {},
   "source": [
    "### Code to Retrieve, Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149f7765-d81a-4b32-a5ba-37390fa5a77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "universe = pd.DataFrame(\n",
    "    data=[\n",
    "        ['Large Cap US Equity', 'IWB'],\n",
    "        ['Small Cap US Equity', 'IWM'],\n",
    "        ['Dev Mkts non-US Equity', 'EFA'],\n",
    "        ['Emerg Mkts Equity', 'EEM'],\n",
    "        ['Global REIT', 'VNQ'],\n",
    "        ['Corp Bonds', 'LQD'],\n",
    "        ['Short-Term Treasury', 'SHY'],\n",
    "    ],\n",
    "    columns=['Segment', 'Ticker']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173f16bb-5856-4765-bf23-1e7e9c42f22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "frames = []\n",
    "for _, row in universe.iterrows():\n",
    "    ticker = row.Ticker\n",
    "    frames.append(pd.read_csv(f'{ticker}_daily.csv').set_index('Date')['Adj Close'].rename(ticker))\n",
    "\n",
    "prices = pd.concat(frames, axis=1)\n",
    "n_obs = prices.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4249a9-9dcf-4aa6-a934-a2004de907df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define frequencies we want to analyze\n",
    "freqs = {\n",
    "    'daily': 1,\n",
    "    'weekly': 5,\n",
    "    'monthly': 21,\n",
    "    'quarterly': 62,\n",
    "    'yearly': 252\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877e17e8-7da2-464d-a881-c06b081b6d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the simple price change for each period\n",
    "returns = {k: prices.iloc[0:n_obs:v, :].pct_change().dropna(axis=0, thresh=prices.shape[1]) for k,v in freqs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab739e74-5fff-465b-8bad-fdca593a649c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for freq, rts in returns.items():\n",
    "freq = 'monthly'\n",
    "def calc_stats(freq, returns):\n",
    "    rts = returns[freq]\n",
    "    period_stats = pd.DataFrame(index=None, columns=rts.columns)\n",
    "    period_stats.loc['Avg Return', :] = rts.mean()\n",
    "    period_stats.loc['Std Dev', :] = rts.std()\n",
    "    period_stats.loc['Return/Risk', :] = period_stats.loc['Avg Return', :] / period_stats.loc['Std Dev', :]\n",
    "    period_stats.loc['Skewness', :] = rts.skew()\n",
    "    period_stats.loc['Kurtosis', :] = rts.kurt()\n",
    "    period_stats.loc['10th Pct', :] = rts.apply(np.percentile, axis=0, q=10)\n",
    "    period_stats.loc['25th Pct', :] = rts.apply(np.percentile, axis=0, q=25)\n",
    "    period_stats.loc['50th Pct', :] = rts.apply(np.percentile, axis=0, q=50)\n",
    "    period_stats.loc['75th Pct', :] = rts.apply(np.percentile, axis=0, q=75)\n",
    "    period_stats.loc['90th Pct', :] = rts.apply(np.percentile, axis=0, q=90)\n",
    "    \n",
    "    stats_styled = period_stats.T.style.format({\n",
    "        'Avg Return': '{:,.2%}',\n",
    "        'Std Dev': '{:,.2%}',\n",
    "        'Return/Risk': '{:,.2}',\n",
    "        'Skewness': '{:,.2}',\n",
    "        'Kurtosis': '{:,.2}',\n",
    "        '10th Pct': '{:,.2%}',\n",
    "        '25th Pct': '{:,.2%}',\n",
    "        '50th Pct': '{:,.2%}',\n",
    "        '75th Pct': '{:,.2%}',\n",
    "        '90th Pct': '{:,.2%}',\n",
    "    })\n",
    "    return stats_styled\n",
    "\n",
    "\n",
    "def plot_corr(freq, returns, lower_only=True):\n",
    "    rts = returns[freq]\n",
    "    cor = rts.corr()\n",
    "    if lower_only:\n",
    "        # create mask to only show bottom triangle of matrix\n",
    "        mask = np.triu(np.ones_like(cor, dtype=bool))\n",
    "        # exclude diagonal (we want to see it)\n",
    "        mask[np.diag_indices_from(mask)] = False\n",
    "    else:\n",
    "        mask = np.zeros_like(rts.corr())\n",
    "    hm = sns.heatmap(cor, mask=mask, cmap='coolwarm', center=0, annot=True)\n",
    "    _ = hm.set_title(f'{freq.capitalize()} Retun Correlations')\n",
    "\n",
    "def plot_time_series(freq, returns):\n",
    "    rts = returns[freq]\n",
    "    rts.add(1).cumprod().add(-1).plot(kind='line', title=f'{freq.capitalize()} Cumulative Returns')\n",
    "\n",
    "def plot_hist(freq, returns, alpha=0.5):\n",
    "    rts = returns[freq]\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    for col in rts.columns:\n",
    "      ax.hist(rts[col], alpha=alpha, bins=50)  # Adjust alpha as needed\n",
    "\n",
    "    ax.legend(rts.columns, loc='best', prop={'size':10}, handletextpad=0.2)  # Adjust legend options \n",
    "    \n",
    "    # Add labels and title\n",
    "    plt.xlabel(\"Return\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(f\"{freq.capitalize()} Return Distribution\")\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf490f08-e351-488f-be12-2173012a92ac",
   "metadata": {},
   "source": [
    "### Monthly Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadf40b9-a4cc-49d1-8574-f55b8cb275ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_stats('monthly', returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b4c8a8-0306-4b3f-bca4-a7923a1f620f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_corr('monthly', returns, lower_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53018335-b4ea-4b08-a097-55db001781c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_time_series('monthly', returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e073c1-2d13-479d-a3c0-45b5dd534ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hist('monthly', returns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c50c15f-4591-43e3-adaf-1069df59967b",
   "metadata": {},
   "source": [
    "### Intro\n",
    "\n",
    "In this exercise I am discussing the preferences of different risk measurement techniques and impact on asset allocation, The summary of investment preferences follows with supporting tables and graphs after that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2512732c-b9c1-4819-91e2-0c65fea67add",
   "metadata": {},
   "source": [
    "### Investment Preferences\n",
    "\n",
    "The investable universe consists of IWB, IWM, EFA, EEM, VNQ, LQD, and SHY ETFs. \n",
    "\n",
    "#### Mean-Variance\n",
    "Mean-variance optimization will prefer assets with a higher level of return per unit of risk and those that lower variance of the overall portfolio by having lower correlations with the other assets in the investment universe. Since higher moments (skewness/kurtosis) are not considered, these statistics will not affect the allocation. \n",
    "- Since defensive (SHY and LQD) assets have low or negative correlation to REITs and equities these will have a significant allocation the MV framework. I suspect SHY will get a higher allocation in the lower target return portfolios and LQD weight will increase as target return move in the same direction. \n",
    "- IWB has the highest return/risk ratio and low correlation to defensive asset classes. I suspect MV approach will allocate a significant portion to this asset. Of course, the actual allocation will greatly very between the different return targets for the portfolio.\n",
    "- IWM, EFA, EEM have middling reutnr/risk ratios and correlations to other asset classes. Since MV allocation tends to pile into a couple of assets (assuming minimal constraints) and not utilize the full asset universe I predict that there will not be a large allocation to these.\n",
    "- I don't foresee the optimizer allocating significant capital to REITs; it has higher correlation to the defensive asset classes (corp. bonds and treasuries) than equities but a lower level of return per unit of risk.\n",
    "\n",
    "#### Mean Absolute Deviation (MAD)\n",
    "\n",
    "Mean-MAD optimization focuses on minimizing the average deviation of returns from the mean, which will emphasize downside protection. This means the mean-MAD optimization will prefer assets that have lower kurtosis (lighter tails) because there are fewer observations with large distance from the mean. Additionally, since the distance from the mean is not squared (unlike mean-variance) mean-MAD will penalize more volatile assets less and prefer diversitying assets with different return patterns.\n",
    "- LQD, VNQ have heavy tails based on kurtosis measures, so this optimization technique is unlikely to allocate a large amount to these assets.\n",
    "- SHY will be preferred due to its diversifying benefit\n",
    "- From equities, IWM will likely to be preferred over IWB due to slightly lower correlation with other assets and lower kurtosis. EFA has similar dispersion (10th and 90th percentiles) to IWB and EEM is similar to IWM, but with lower average return and similar correlations. I don't see a significant allocation to these 2 asset classes.\n",
    "   \n",
    "### Conditional Value at Risk (CVaR)\n",
    "\n",
    "Mean-CVaR is similar to mean-MAD in the sense of minimizing deviation of returns, but it focuses specifically on the downside. So assets with a heavy left tails will not be favored by this approach. To put it in terms of our data tables, negatively skewed assets will be penalized by this approach. \n",
    "- LQD, VNQ have negative skweness and therefore a higher CVaR. These will not get a high allocation.\n",
    "- IWB and EFA also have negative skewness and therefore will be penalized more by this approach.\n",
    "- IWM, EEM, and SHY have positive skewness and the optimizer should favor these assets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d581e13b-21d8-455f-b975-fc889fec729e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b93b0c-ad5c-4dc9-ab2b-c4735fd17217",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2a3359-f34d-4d70-8351-c02b2f83e70c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69a5d6d-463d-414e-bdc2-26b8592107cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_min_var(mean, cov, w, constraints, labels):\n",
    "    \"\"\"\n",
    "    Helper function that will be used throughout the homework\n",
    "    \"\"\"\n",
    "    # minimize variance of portfolio\n",
    "    obj = cp.Minimize(cp.quad_form(w, cov))\n",
    "    \n",
    "    prob = cp.Problem(\n",
    "        objective=obj,\n",
    "        constraints=constraints,\n",
    "    )\n",
    "    \n",
    "    prob.solve(solver=cp.ECOS)\n",
    "    print(prob.status)\n",
    "    assert prob.status == 'optimal'\n",
    "\n",
    "    p_var = w.value @ cov @ w.value.T\n",
    "    p_risk = np.sqrt(p_var)\n",
    "    p_ret = w.value @ mean\n",
    "    print('\\nPortfolio Weights:\\n')\n",
    "    print(pd.Series(index=labels, data=np.round(w.value * 1e2, 2), name='Weight'))\n",
    "    print(f'\\nPortfolio risk: {round(p_risk * 1e2, 2)}%')\n",
    "    print(f'Portfolio return: {round(p_ret * 1e2, 2)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c92084-0230-4719-b121-fd32ba532dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = 'yearly'\n",
    "rts = returns[freq]\n",
    "mean = rts.mean()\n",
    "cov = rts.cov()\n",
    "\n",
    "# define the vector we're solving\n",
    "w = cp.Variable(len(mean))\n",
    "\n",
    "constraints = [\n",
    "    # sum of all weights is one\n",
    "    cp.sum(w) == 1,\n",
    "    # all weights non-negative\n",
    "    w >= 0,\n",
    "    w @ mean >= (0.08 * (freqs[freq]/252)),\n",
    "]\n",
    "\n",
    "solve_min_var(mean.values, cov.values, w, constraints, labels=list(rts.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4e5b84-3e8b-4e83-aa40-2bd83fb2d5f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
